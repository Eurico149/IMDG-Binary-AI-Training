{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch, os, re"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the data from IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ],
   "id": "f76193c792a10070",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Uses the 'distilbert-base-uncased' pretrained model to tokenize the text data\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(example):\n",
    "    # Takes a text and return the tokenized version of it, with exactly 512 tokens\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Takes each line from the dataset applies the tokenize function and stores the results\n",
    "tokenized = dataset.map(tokenize, batched=True)"
   ],
   "id": "71e081375f41903",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creates the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ],
   "id": "515aa969f533bb96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Training Config\n",
    "- Checkpoints generated for each epoch\n",
    "- Checkpoints saved at ./results\n",
    "- Saves the last checkpoint\n",
    "- Trains for 2 epochs\n",
    "- Half precision memory when using GPU\n",
    "- Batch size 16 for train and evaluation per device\n",
    "- Uses 4 Workers to load data in parallel\n",
    "- Accumulate gradient for over 2 batches\n",
    "\"\"\"\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    fp16=cuda,\n",
    "    dataloader_num_workers=4,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "if cuda:\n",
    "    training_data = tokenized[\"train\"].shuffle()\n",
    "    evaluation_data = tokenized[\"test\"].shuffle()\n",
    "else:\n",
    "    training_data = tokenized[\"train\"].shuffle().select(range(700))\n",
    "    evaluation_data = tokenized[\"test\"].shuffle().select(range(300))\n",
    "\n",
    "# Selects the training data, test data, training config and tokenizer model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=evaluation_data\n",
    ")\n",
    "\n",
    "# Starts the training\n",
    "trainer.train()"
   ],
   "id": "476103e1eafb5bf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Metric used for measuring the efficiency of the model\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Initiate the evaluation\n",
    "trainer.compute_metrics = compute_metrics\n",
    "trainer.evaluate()"
   ],
   "id": "7264c28a52e20c99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Returns the path of last checkpoint\n",
    "def get_latest_checkpoint(results_dir=\"./results\"):\n",
    "    checkpoint_dirs = [\n",
    "        d for d in os.listdir(results_dir)\n",
    "        if re.match(r\"^checkpoint-\\d+$\", d)\n",
    "    ]\n",
    "\n",
    "    if not checkpoint_dirs:\n",
    "        return None\n",
    "\n",
    "    latest = max(checkpoint_dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "    return os.path.join(results_dir, latest)"
   ],
   "id": "362498e24bdd12a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_dir = get_latest_checkpoint()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "text = input()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "inputs = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "print(f\"Result: {bool(predicted_class)} \\nProbabilities: {probabilities.tolist()[0]}\")"
   ],
   "id": "a268ff2ad0bdebe6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
